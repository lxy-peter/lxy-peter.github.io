<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Xing Yi (Peter) Liu</title>
  
  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Xing Yi (Peter) Liu</name>
              </p>
              <p>I received my M.S. in Computer Science from Columbia University and conducted research at Tsinghua University's Institute for AI Industry Research in 2023.
              </p>
              <p>
                My research interest is in multimodal learning, efficient machine learning, and applications of deep learning.
              </p>
              <p style="text-align:center">
                <a href="mailto:liu.peter@columbia.edu">liu.peter@columbia.edu</a> &nbsp|&nbsp
                <a href="data/cv.pdf" target="_blank">CV</a> &nbsp|&nbsp
                <a href="https://scholar.google.com/citations?user=J10jvjMAAAAJ&hl=en" target="_blank">Google Scholar</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/XingYiLiu.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/XingYiLiu.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Publications</heading>
            <!--
            <p>
              Representative papers are <span class="highlight">highlighted</span>.
            -->
            </p>
          </td>
        </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

      
          <!-- <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/mira_before.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Efficient Ensemble Architecture for Multimodal Acoustic and Textual Embeddings in Punctuation Restoration using Time-Delay Neural Networks</papertitle>
              <br>
              <a href="https://yenchenlin.me/">Lin Yen-Chen</a>, 
              <a href="http://www.peteflorence.com/">Pete Florence</a>, 
              <a href="https://andyzeng.github.io/">Andy Zeng</a>, <strong>Jonathan T. Barron</strong>, 
              <a href="https://yilundu.github.io/">Yilun Du</a>
              <br>
              <em>CoRL</em>, 2022
              <p></p>
              <p>
                NeRF lets us synthesize novel orthographic views that work well with pixel-wise algorithms for robotic manipulation.
              </p>
            </td>
          </tr> -->

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='images/2023kedd.png' width="160">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="papers/2023kedd.pdf" target="_blank"><papertitle>Towards Unified AI Drug Discovery with Multiple Knowledge Modalities</papertitle></a>
                <br>
                Yizhen Luo*, <strong>Xing Yi Liu</strong>*, Kai Yang, Kui Huang, Massimo Hong, Jiahuan Zhang, Yushuai Wu, Zaiqing Nie
                <br>
                <em>arXiv 2023</em>
                <br>
                <a href="data/2023kedd.bib", target="_blank">BibTeX</a> | 
                <a href="https://arxiv.org/abs/2305.01523", target="_blank">arXiv</a>
                <p></p>
                <p>
                   Human pharmaceutical experts draw insight from both structured knowledge in knowledge bases and unstructured knowledge in biomedical literature. Our proposal, KEDD, extracts knowledge similarly and solves a variety of drug discovery tasks in a unified framework.
                </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='images/2023molfm.png' width="160">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="papers/2023molfm.pdf" target="_blank"><papertitle>MolFM: A Multimodal Molecular Foundation Model</papertitle></a>
                <br>
                Yizhen Luo, Kai Yang, Massimo Hong, <strong>Xing Yi Liu</strong>, Zaiqing Nie
                <br>
                <em>arXiv 2023</em>
                <br>
                <a href="data/2023molfm.bib", target="_blank">BibTeX</a> | 
                <a href="https://arxiv.org/abs/2307.09484", target="_blank">arXiv</a>
                <p></p>
                <p>
                  We introduce MolFM, a multimodal molecular foundation model designed to facilitate joint representation learning from molecular structures, biomedical texts, and knowledge graphs. We propose cross-modal attention to facilitate comprehension between these modalities.
                </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='images/2023efficient.png' width="160">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="papers/2023efficient.pdf" target="_blank"><papertitle>Efficient Ensemble Architecture for Multimodal Acoustic and Textual Embeddings in Punctuation Restoration using Time-Delay Neural Networks</papertitle></a>
                <br>
                <strong>Xing Yi Liu</strong>, Homayoon Beigi
                <br>
                <em>Recognition Technologies 2023</em>
                <br>
                <a href="data/2023efficient.bib", target="_blank">BibTeX</a> | 
                <a href="https://arxiv.org/abs/2302.13376", target="_blank">arXiv</a>
                <p></p>
                <p>
                  EfficientPunct outperforms the previous best punctuation restoration model by 1.0 F1 points, using less than a tenth of its parameters to process embeddings. We create an ensemble streamlined from a speech recognizer to extract audio embeddings.
                </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/2016uwnlp.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="papers/2016uwnlp.pdf" target="_blank"><papertitle>UWNLP at the NTCIR-12 Short Text Conversation Task</papertitle></a>
              <br>
              Anqi Cui, Guangyu Feng, Borui Ye, Kun Xiong, <strong>Xing Yi Liu</strong>, Ming Li
              <br>
              <em>NTCIR-12 2016</em>
              <br>
              <a href="data/2016uwnlp.bib", target="_blank">BibTeX</a>
              <p></p>
              <p>
                Our submission to the NTCIR-12 task treats short text conversation as a community question-answering problem. We achieved performances of mean nDCG@1 0.2767, mean P+ 0.4284 and mean nERR@10 0.4095.
              </p>
            </td>
          </tr>

        </tbody></table>
        


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Unpublished and Current Work</heading>
          </td>
        </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='images/unpub-2023punctuation.jpg' width="160">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Improving the Efficiency of Multimodal Punctuation Restoration</papertitle>
                <br>
                Supervisor: Prof. Homayoon Beigi, Columbia University
                <br>
                <p></p>
                <p>
                  We are extending our work on EfficientPunct, our previous punctuation model, by performing ablation studies and further shrinking model size. We experimented with linear discriminant analysis for dimensionality reduction of multimodal features and are studying the effect of jointly training multimodal fusion layers with the acoustic and/or text encoders.
                </p>
            </td>
        </tr>

      </tbody></table>

        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                Thank you to <a href="https://jonbarron.info/">Jon Barron</a> for this website's template.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
