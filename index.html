<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Xing Yi (Peter) Liu</title>
  
  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Xing Yi (Peter) Liu</name>
              </p>
              <p>I received my M.S. in Computer Science from Columbia University and my B.A.Sc. from the University of California, Los Angeles. I am currently looking for PhD opportunities in Beijing or Shanghai.
              </p>
              <p>
              My research interest is in multimodal deep learning and efficient machine learning.
              </p>
              <p style="text-align:center">
                <a href="mailto:liu.peter@columbia.edu">liu.peter@columbia.edu</a> &nbsp|&nbsp
                <a href="data/cv.pdf" target="_blank">CV</a> &nbsp|&nbsp
                <a href="https://scholar.google.com/citations?user=J10jvjMAAAAJ&hl=en" target="_blank">Google Scholar</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/XingYiLiu.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/XingYiLiu.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Publications</heading>
            <p>
              Representative papers are <span class="highlight">highlighted</span>.
            </p>
          </td>
        </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

      
          <!-- <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/mira_before.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Efficient Ensemble Architecture for Multimodal Acoustic and Textual Embeddings in Punctuation Restoration using Time-Delay Neural Networks</papertitle>
              <br>
              <a href="https://yenchenlin.me/">Lin Yen-Chen</a>, 
              <a href="http://www.peteflorence.com/">Pete Florence</a>, 
              <a href="https://andyzeng.github.io/">Andy Zeng</a>, <strong>Jonathan T. Barron</strong>, 
              <a href="https://yilundu.github.io/">Yilun Du</a>
              <br>
              <em>CoRL</em>, 2022
              <p></p>
              <p>
                NeRF lets us synthesize novel orthographic views that work well with pixel-wise algorithms for robotic manipulation.
              </p>
            </td>
          </tr> -->

          <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='images/2023efficient.png' width="160">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="papers/2023efficient.pdf" target="_blank"><papertitle>Efficient Ensemble Architecture for Multimodal Acoustic and Textual Embeddings in Punctuation Restoration using Time-Delay Neural Networks</papertitle></a>
                <br>
                <strong>Xing Yi Liu</strong>, Homayoon Beigi
                <br>
                <em>Recognition Technologies Technical Report 2023, in review for Interspeech 2023</em>
                <br>
                <a href="data/2023efficient.bib", target="_blank">BibTeX</a> | 
                <a href="https://arxiv.org/abs/2302.13376", target="_blank">arXiv</a>
                <p></p>
                <p>
                  EfficientPunct outperforms the previous best punctuation restoration model by 1.0 F1 points, using less than a tenth of its parameters to process embeddings.
                </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/2016uwnlp.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="papers/2016uwnlp.pdf" target="_blank"><papertitle>UWNLP at the NTCIR-12 Short Text Conversation Task</papertitle></a>
              <br>
              Anqi Cui, Guangyu Feng, Borui Ye, Kun Xiong, <strong>Xing Yi Liu</strong>, Ming Li
              <br>
              <em>NTCIR-12 2016</em>
              <br>
              <a href="data/2016uwnlp.bib", target="_blank">BibTeX</a>
              <p></p>
              <p>
                Our submission to the NTCIR-12 task treats short text conversation as a community question-answering problem.
              </p>
            </td>
          </tr>

        </tbody></table>







        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Unpublished and Current Work</heading>
          </td>
        </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <tr>
            <td style="padding:20px;width:0%;vertical-align:middle">
                <div class="one">
                  <img src='images/unpub-2023peptide.jpg' width="160">
                </div>
              </td>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <papertitle>Peptide Quantification</papertitle>
                <br>
                Supervisor: Prof. Ming Li, University of Waterloo
                <br>
                <p></p>
                <p>
                  Our aim is to determine relative abundances of specific peptides in biological samples using machine learning. We adopt PointIso for peptide feature detection from liquid chromatography mass spectrometry maps and aggregate varying peptide amounts detected in different replicates of the same dilution sample.
                </p>
            </td>
        </tr>


        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='images/unpub-2023punctuation.jpg' width="160">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Improving the Efficiency of Multimodal Punctuation Restoration</papertitle>
                <br>
                Supervisor: Prof. Homayoon Beigi, Columbia University
                <br>
                <p></p>
                <p>
                  We are extending our work on EfficientPunct, our previous punctuation model, by performing ablation studies and further shrinking model size. We experimented with linear discriminant analysis for dimensionality reduction of multimodal features and are studying the effect of jointly training multimodal fusion layers with the acoustic and/or text encoders.
                </p>
            </td>
        </tr>


        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='images/unpub-2023descriptive.png' width="160">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Descriptive Semantic Image Translation with FlexIT</papertitle>
                <br>
                Supervisor: Prof. Iddo Drori, Columbia University
                <br>
                <p></p>
                <p>
                  We proposed a novel framework for translating objects in images using natural language descriptors. We extended FlexIT (from CVPR 2022) by introducing a MDETR component for multimodal object detection. We achieved an LPIPS similarity score of 10.4 on the COCO-Stuff dataset, superior to FlexIT‚Äôs 24.7.
                </p>
            </td>
        </tr>

      </tbody></table>

        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                Thank you to <a href="https://jonbarron.info/">Jon Barron</a> for this website's template.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
